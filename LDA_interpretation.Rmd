---
title: "LDA Interpretation"
author: "Jann Goschenhofer"
date: "January 2018"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc-depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Summary of models and especially their interpretation (graphically as well as content based) used in Survival Analysis. This document emerged throughout the exam preparation for a lecture on Survival Data Analysis at LMU in winter 2018. Most examples are based on that lecture taught by Prof. Kuechenhoff and Andreas Bender. 


```{r, echo=FALSE, warnings = FALSE, message = FALSE}
setwd("C:/Users/Martin/Documents/survival-master")

library(ggplot2)
theme_set(theme_bw())
library(data.table)
library(magrittr)
library(pec)
library(broom)
library(tidyr)
library(dplyr)
library(mgcv)
library(purrr)
library(survival)
library(timereg)
# devtools::install_github("adibender/ldatools")
library(ldatools)
# devtools::install_github("adibender/pammtools")
library(pammtools)

```

# Censoring

1. __Right__:  
    1. Type 1: study ends before event occured. E.g.: fixed time study of 1 year
    2. Type 2: ...
    3. Type 3: person withdraws from study because of other event. E.g.: interest on cancer death, person is getting shot
2. __Left__: we know when event occurs but we do not know when it started. E.g.: person dies at week 4 on cancer but we don't know the time of the disease outbreak. Our observed survival time of 4 weeks is thus equal (best case) or smaller then the observed. 
3. __Left Truncation__: Biased because only people that survived made it to the study. E.g.: deductible in insurances, people with losses < deductibles are not getting observed. 


# Kaplan Meier



### Model Equation

Cannot simply $1-F(t)$ due to censoring. KM takes that into account. 

Estimate the __Survival rate__ non-parametrically without any covariables: 

$$
\hat S(t) = \prod_{t_k \leq t}(1-d_k / n_k), \forall t \geq t_1
$$
where $d_k =$ number of events at time point $t_k$ (neither dead nor censored) and $n_k = $ amount of people under risk right before time $t_k$.

Reveals a step function with jumps at each $t_k$ where events took place. 

### Data 

This is some random SOEP data and we estimate Survival functions for both genders: 

```{R, echo = FALSE}

## Read in data
soep <- read.csv("data/soep2000.csv", header = TRUE, sep = ";") # Einlesen Daten
head(soep)

```

### Model

```{R, echo = FALSE}

par(mar=c(5,5,4,4))
## Estimate the survivor function for duration of unemployment separately for
## each gender (Kaplan-Meier)
## Note: Here the event is something positive (end of unemployment),
# thus high survival rates have negative meaning (i.e. staying unemployed longer)
km_sex <- survfit(Surv(dauer, status) ~ female, data = soep)

plot(km_sex, lwd = 2,
  xlab = "Time in months t", las = 1,
  ylab = expression(hat(S)[KM](t)), lty = c(1, 2),
  main = "Duration of unemployment by gender\n (Kaplan-Meier estimator)")

legend("topright", legend = c("male", "female"), lty = c(1, 2))
## Comparison of the curves:
## After how many months are 50% of the persons no longer unemployed?
abline(h = 0.5, lty = 3)
```

This gives incidence for the Proportional Hazards assumption as survival curves are more or less parallel. 


### Test

Plotting estimated confidence intervals __DOES NOT ENABLE__ us to interpret signifiance. KI's can cross, and still there is a significant effect. 

* Only interpret the p-value of the log rank test!
* log rank test resembles the score test in the cox model. 
* ````surfdiff()``` for p = 2 > 1 variables: H0: no differencies across 4 resulting groups. If $p < \alpha:$ reject H0.  

```{R, echo = FALSE}

## Log-rank test for comparing the two groups
# in default mode rho=0 this computes the log-rank test, muchacho
survdiff(Surv(dauer, status) ~ female, data = soep)

```




# Nelson Aalen

# Accelerated Failure Time Transformation models

Assumptions: 
1. covariates have a multiplicative effect on the __Survival time__. E.g.: Survival time for smokers is an accelerated version of the survival time for non-smokers.   
2. the survival time follows an assumed distribution that we get applying a density transformation

We model the survival time directly with the log-trafo:

$$
log(T) = Y = \beta_0 + X^T\beta + \sigma \epsilon
$$

$$
T = exp(Y) = exp(\beta_0) * exp(X^T\beta) * exp(\sigma \epsilon)
$$
$$
\text{with } \epsilon \sim \text{Distribution e.g.: SEV, Normal, logistic, .....}  
$$
Thus, the effect of the estimated coefficient $\hat\beta_j$ on Survival time T is $exp(\beta_j)$

Steps for the estimation:
1. calculate density for T 
2. classic Maximum Likelihood Estimation



## Examples for all AFT's 

The exponential and the weibull AFT can be compared with a Cox PH model as they also have proportional (time independent) hazard ratios. This means that the following re-parametrization holds:
$$
\beta_{PH} = - \frac{\beta_{\text{AFT: WB or Exp}}}{\sigma}
$$
where $sigma$ is our scale parameter from the AFT model equation.   

Therefore we compare with this baseline Cox model:

```{R, echo = FALSE}
## Cox PH model
coxphFit <- coxph(Surv(futime, fustat) ~ ecog.ps + rx, ovarian)
summary(coxphFit)
```


### Exponential

```{R, echo = FALSE}
## Exponential model
survregExp <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian,
  dist = "exponential")
summary(survregExp)
```

#### AFT interpretation:

* Geometric mean of survival time: 1055  
* 1 unit change in ecog.ps shortens survival time by exp(-0.433) = 0.65  
* 1 unit change in rx increases survival time by exp(0.582) = 1.79  
* though, both effects are non significant  

```{R, echo = FALSE}
exp(coef(survregExp))
```

#### PH interpretation (invert the coefficients)
* 1 unit change in ecog.ps increases the hazard h(t) by 1/exp(0.433) = 1.54  
* 1 unit change in rx decreases h(t) by 0.56  

```{R, echo = FALSE}
exp(-1 * coef(survregExp))
```


### Weibull


```{R, echo = FALSE}
## Exponential model
survregWB <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian,
  dist = "weibull")
summary(survregWB)
```

#### AFT interpretation:

* Geometric mean of survival time: 988 
* 1 unit change in ecog.ps shortens survival time by exp(-0.385) = 0.68  
* 1 unit change in rx increases survival time by exp(0.529) = 1.70 
* though, both effects are non significant  
* If scale parameter ```{R, echo = FALSE} survregWB$scale ``` was close to 1 we would yield an exponential model. Our shape parameter is 1 / scale
* coefficients: ```{R, echo = FALSE} exp(coef(survregWB)) ```


#### PH interpretation (multiply by -1 and the shape parameter before exp())
* 1 unit change in ecog.ps increases the hazard h(t) by 1.55 
* 1 unit change in rx decreases h(t) by 0.55

```{R, echo = FALSE}
shapeParameter <- 1 / survregWB$scale
exp(-1 * shapeParameter * coef(survregWB))
```

### Log Normal

Only AFT Interpretation!

* 1 unit incrase in ecog.ps shortens survival time by exp(-.229) = 0.79
* 1 unit incrase in rx increases survival time by exp(0.813) = 2.25
* Can we interpret the scale parameter?
* \textcolor{red}{MORE TO ADD! DISCUSS}

```{R, echo = FALSE}
## Log Normal model
survregLogNormal <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist = "lognormal")
summary(survregLogNormal)
```

### Log logistic


```{R, echo = FALSE}
## Log Logistic model
survregLogLogistic <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist = "loglogistic")
summary(survregLogLogistic)

```

## Weibull Model

### Model equation 

model Survival time directly

$$
T = exp(Y) = exp(\beta_0) * exp(X^T\beta) * exp(\sigma \epsilon)
$$
$$
\text{with } \epsilon \sim \text{Distribution e.g.: SEV, Normal, logistic, .....}  
$$
Then use density-transformation to get distribution for T (Survival time). 

Steps:
1. calculate density for T 
2. classic Maximum Likelihood Estimation

### Data 

where delta depicts the event indicator (delta = 1: non-censored, delta = 0: censored)

```{r, echo = FALSE}

## Read in data
data("tongue", package = "KMsurv")
tongue <- tongue %>% mutate(type = as.factor(type))
head(tongue)
```

### Model

```{R, echo = FALSE}
## AFT: Weibull model
weibull.tongue <- survreg(Surv(time, delta) ~ as.factor(type), dist = "weibull",data = tongue)
summary(weibull.tongue)
```

* Survival time decreases by factor exp(-0.669) = 0.512 for patient with type 2 w.r.t. type 1 given constant other features 
* Not siginificant with $p = 0.056$
* Failure time accelerates by factor 1 / 0.512 = 1.953125 \textcolor{red}{DISCUSS}
* Scale Parameter = 1.24 -> if it was 1.0 we would yield an exponential distributed model. This would yield $\lambda{t} = \lambda$, constant and independent of time. 
* Weibull model: not constant $\lambda(t) = \lambda * \alpha (\lambda*t)^{\alpha - 1}$ but hazard ratio still independent of time -> PH assumption

# Cox Regression model

Estimates coefficents $\beta$ that have multiplicative effect on time-dependent hazard $\lambda_0(t)$. The baseline hazard is estimated non-parametrically via Breslow estimate. Thus, we yield step-functions for visualization, estimation, ...

[super sweet R-bloggers post on Cox models](https://www.r-bloggers.com/cox-model-assumptions/)


### Model equation

$$
\lambda_i(t) = \lambda_0(t) exp(x_i'\beta)
$$


To get the estimator for the cumulative Hazard and the Survival rate:  

1. estimate $\beta$s via Cox __parametrically__    
1. estimate non-parametrically baseline hazards $\lambda_0(t)$ e.g. via Breslow __non-parametrically__  
1. calculate for each t $\lambda(t) = \lambda_0(t) exp(x_i'\beta)$    
1. cumulate the $\lambda(t)$ to the cumulative Hazards $\Lambda_{t} = \sum_{i = 1}^t \lambda_i$. ```basehaz()``` plottet $\Lambda_0$  
1. calculate estimated Survival $S(t) = exp(-\Lambda_t)$    
Therfore Cox PH model is termed __semi parametric__.   

### Data

where delta depicts the event indicator (delta = 1: non-censored, delta = 0: censored)

```{r, echo = FALSE}

## Read in data
data("tongue", package = "KMsurv")
tongue <- tongue %>% mutate(type = as.factor(type))
head(tongue)
```


### Model

We are searching for the effect of the binary treatment type. 

* Person with type 2 has a multiplicative factor exp(0.4664) = 1.594245 higher hazard rate than a person with type 1 (ceteris paribus in case of other covariates)  
* this effect is not significant as the H0 can not be rejected at $\alpha = 0.05$, REMIND but this does not imply testing of the PH assumption  
* (log rank-) score test: tests for significant differencies in the survival curves for the two   subpopulations seperated by the __categorical variable__ of interest (here: treatment). This means that the probability of an event occurring at any time point is the same for each subpopulation. H0: they do not differ -> p > 0.05: H0 cannot be rejected -> no significant effect of treatment. If there are more than 1 categorical variablewe have the H0: no effect of no covariate at all. Reject again if $p < \alpha$
* ````surfdiff()``` for p = 2 > 1 variables: H0: no differencies across 4 resulting groups. If $p < \alpha:$ reject H0.  
* Partial likelihood test: for __continuos__ variables!
\textcolor{red}{WHAT HAPPENS WITH MORE COVARIATES? E.G.: one significant, the other not}

Summary of the Cox-PH model:

```{r, echo = FALSE}
## Cox-PH model
cox.tongue <- coxph(Surv(time, delta) ~ type, data = tongue)
summary(cox.tongue)

```

### Test the Cox PH assumption for the covariates

#### Graphically

The scaled Schoenfeld residuals are used for that test and plotted against the time. Do this for each covariate to check the PH assumption for each covariate. If they __randomly and unstructured__ center around zero: PH assumption holds! If not, not. The plot estimates a smooth function of the residuals over time for better visualization. Holds here:


```{r, echo =FALSE}
plot(cox.zph(cox.tongue))
```

#### Test PH

Also based on Schoenfeld residuals, not exam-relevant. If $p >> 0.05$ there is no violation of the PH. 

### Test overall fit

Plot Cox-Snell residuals vs. Cumulated Hazard. If they share the diagnonal, everything is fine and we have a good overall model fit. 

```{r, echo = FALSE}
cox.res.cs <- tongue$delta - residuals(cox.tongue, type = "martingale")

cox.res.surv <- survfit(Surv(cox.res.cs, tongue$delta) ~ 1) %>%
  broom::tidy() %>%
  mutate(
    Lambda = -log(estimate),
    F      = pexp(time)) %>%
  rename(cs.residual = time)

## plot empirical F for censored observations vs. cs residuals (and compare to Exp(1))
# ggplot(cox.res.surv, aes(x=cs.residual, y=1-estimate)) +
#   geom_point() +
#   geom_step() +
#   geom_line(aes(y=F), col=2) +
#   ylim(c(0,1)) + ylab(expression(F(r[i]^{CS}))) + xlab(expression(r[i]^{CS}))

## usual visualization of cs residuals vs. cumulative hazard (for censored sample)
ggplot(cox.res.surv, aes(x=cs.residual, y=Lambda)) +
  geom_point() +
  geom_step() +
  geom_abline(intercept=0, slope=1, col=2) + # add bisector (Winkelhalbierende)
  ylab(expression(hat(Lambda)(r[i]^{CS}))) + xlab(expression(r[i]^CS))

```

# Model fit Analysis

## Prediction Error Curves (PEC)

The predicted survival time for each time point is compared with the true survival time within the __Brier Score__. Some magic is added such as _inverse probability of censoring weights (IPCW)_ to account for right censoring. Then scores for each time point are computed using Cross-Validation and the Brier Scores over time are plotted for all desired models. The lower the score, the better. This method is __model agnostic__.  

For Melanoma compare predictive performance of Cox model with only variable ulcer as predictor with the reference Kaplan-Meier estimates and a Cox-PH model that uses year as a linear predictor. We see, that our cox-model outperforms the simple Kaplan-Meier estimator (which does not use any variables) and both outperform the stupid Cox model with time as linear predictor.     

```{R, echo = FALSE, message = FALSE, warnings = FALSE}
library(survival)
library(pec)
data("Melanoma", package="MASS")
cox_ulcer <- coxph(Surv(time, status ==1) ~ ulcer, data = Melanoma)
## Estimate CV PEC for even grid of time points between 0 and 4000
pec_obj <- pec::pec(list(cox_ulcer = cox_ulcer),
  formula = Surv(time, status == 1) ~ year + ulcer, 
  data = Melanoma, exact = FALSE, 
  times = seq(0, 4000, by = 200),
  splitMethod = "cv10", B = 20, 
  ipcw.refit = TRUE, 
  reference = TRUE)

mod_list <- list(cox_ulcer = cox_ulcer,
  cox_year = update(cox_ulcer, .~. -ulcer + year))
pec_obj2 <- pec::pec(mod_list, Surv(time,status==1)~year + ulcer,
  data =  Melanoma, times = seq(0,4000, by=200),
  splitMethod = "cv10", B = 20, ipcw.refit = TRUE, exact = FALSE)
plot(pec_obj2)
```




## Residuals

* Schoenfeld 
* Martingale
* Deviance
* Cox-Snell

### Schoenfeld Residuals $s_{i,j}$

Use case: test PH assumption for each covariate

Idea: compute Schoenfeld residuals for Variable $k$ and $m$ observations. Those residuals should be independent of the survival time. This is the test that ```cox.zph()``` performs. 

PH: effects of covariates are proportional and thus, time invariant. Thus, check for timely structure in residuals, if some timely structure _is left in the residuals_, the models assumption failed. 


#### Test

```{R, echo = FALSE, cache = TRUE}

### DATA 

prison = read.table("http://socserv.mcmaster.ca/jfox/Books/Companion/data/Rossi.txt")%>%
  mutate(subject=row_number())

# transform data to binary

for (column in colnames(prison)) {
  if (is.factor(prison[, column])) {
    
    levels(prison[, column]) = c(levels(prison[, column]), 1, 0)
    
    # for employment data
    prison[, column] = replace(prison[, column], prison[, column] == "yes", 1)
    prison[, column] = replace(prison[, column], prison[, column] == "no", 0)

    # race
    prison[, column] = replace(prison[, column], prison[, column] == "black", 1)
    prison[, column] = replace(prison[, column], prison[, column] == "other", 0)

    # mar
    prison[, column] = replace(prison[, column], prison[, column] == "married", 1)
    prison[, column] = replace(prison[, column], prison[, column] == "not married", 0)
    
    # delete empty levels 
    # prison[, column] = as.factor(as.character(prison[, column]))
    # try numeric values, YES this is it
    prison[, column] = as.integer(as.character(prison[, column]))
  }
}

## transform to long format to accommodate time-varying covariates
prison.long <- prison %>%
  gather(calendar.week, employed, emp1:emp52) %>%
  filter(!is.na(employed)) %>% # employed unequal to NA only for intervals under risk
  group_by(subject) %>%
  mutate(
    start = row_number()-1L,
    stop = row_number(),
    arrest = ifelse(stop==last(stop) & arrest==1, 1, 0),
    calendar.week = as.integer(sub("emp", "", calendar.week))) %>%
  select(subject, calendar.week, start, stop, arrest, employed, fin:educ) %>%
  arrange(subject, start, stop) %>%
  ungroup()

prison.long %<>% mutate(employed.lag1 = lag(employed, default=1))

prisss = prison.long %>%
  mutate(employed.lag1 = lag(employed, default = 0))

m4 <- coxph(Surv(start, stop, arrest) ~ fin + age + prio + mar + employed.lag1, data = prison.long)

zph.prison <- cox.zph(m4, transform="identity")
zph.prison
```

Small p-value for variable age indicates problem with the PH assumption here. High value for employed.lag1 indicates nice fulfillment of PH assumption. 

Can we observe this graphically?

#### Graphically

Plot the Schoenfeld residuals for variable age:

```{R, echo = FALSE}

dfage = as.data.frame(cbind(zph.prison$x, zph.prison$y[,2]))
colnames(dfage) = c("time", "ageresidual")

# smallest p-value for age variable, GLOBAL p = 0.094
schaledsch.m4 <- get_scaledsch(m4, transform="identity")
# check age
ggplot(filter(schaledsch.m4, variable=="age"), aes(x=time, y=residual)) +
  geom_point(alpha=0.7) +
  geom_smooth(method="gam", formula=y~s(x)) +
  geom_hline(yintercept = 0, lty=2, col=2) +
  ggtitle(label = "Scaled Schoenfeld residuals for variable age")

```

PH assumption violated because there is non linear structure in the data. 

What can we do?  

1. Exclude variable   
1. __additionally__ model time varying effect as e.g. $x_{age}\cdot log(1+t)$  
1. non-linearly e.g. using splines  

Check variable ```employed lag1``` that had huge p-value in zph test (good sign for PH):

```{R, echo = FALSE}
# check employed lag
ggplot(filter(schaledsch.m4, variable=="employed.lag1"), aes(x=time, y=residual)) +
  geom_point(alpha=0.7) +
  geom_smooth(method="gam", formula=y~s(x)) +
  geom_hline(yintercept = 0, lty=2, col=2) +
  ggtitle(label = "Scaled Schoenfeld residuals for variable employed.lag1")

```

We see what we expected: there seems to be no PH violation. Sweet!

###Martingale residuals $m_i$
$m_i=\delta_i-r_i$
Use case: determine the functional form to be used for a given covariate.

Measure of the difference between expected and observed events. Takes values between $-\infty$ and +1.
$\sum^n_{i=1}m_i=0$
plot residuals against a single covariate. A smoothed fit is used. If the plot is linear no transformation of the covariate is needed. If there is a threshold, a discretization may be in order.

###Deviance residuals $d_i$
Use case: Check for outliers. Assess the effect of a given individual on the model. 

Idea: One could use martingale residuals , but they are highly skewed. The logarithm inflates values of the martingale residual close to 1 and shrinks large negative values. This leads to a more normally shaped distribution. 

Plot $d_i$ versus the risk scores $x'\beta$
(it's from the book page 381. the slides say something different, but it doesn't make a lot of sense imo...)

### Cox Snell residuals $r_i$

Use case: Check overall goodness of fit

#### Graphically 

H0: Model works - Cox-snell residuals should follow an Exp(1) distribution. If the cox-snell-residuals distribution deviates strongly from the Exp(1), the model does not fit well. 

#### Test

```{R, echo = FALSE}

```



#### Model 

#### Example

Check the overall goodness of fit for a simple cox model: 

```{R, echo  = FALSE, cache = TRUE}
data("tongue", package = "KMsurv")
tongue <- tongue %>% mutate(type = as.factor(type))
# or:
#tongue <- read.table("tongue.dat", header = TRUE)
#head(tongue)
#str(tongue)

## Cox-PH model
cox.tongue <- coxph(Surv(time, delta) ~ type, data = tongue)
summary(cox.tongue)

cox.res.cs <- tongue$delta - residuals(cox.tongue, type = "martingale")


# survfit estimate the 1 - F, where F is the empirical distribution estimate for
# the censored sample of cox snell residuals

cox.res.surv <- survfit(Surv(cox.res.cs, tongue$delta) ~ 1) %>%
  broom::tidy() %>%
  mutate(
    Lambda = -log(estimate),
    F      = pexp(time)) %>%
  rename(cs.residual = time)

## plot empirical F for censored observations vs. cs residuals (and compare to Exp(1))
ggplot(cox.res.surv, aes(x=cs.residual, y=1-estimate)) +
  geom_point() +
  geom_step() +
  geom_line(aes(y=F), col=2) +
  ylim(c(0,1)) + ylab(expression(F(r[i]^{CS}))) + xlab(expression(r[i]^{CS}))

## usual visualization of cs residuals vs. cumulative hazard (for censord sample)
ggplot(cox.res.surv, aes(x=cs.residual, y=Lambda)) +
  geom_point() +
  geom_step() +
  geom_abline(intercept=0, slope=1, col=2) + # add bisector (Winkelhalbierende)
  ylab(expression(hat(Lambda)(r[i]^{CS}))) + xlab(expression(r[i]^CS))
```

Two options:
1. Plot cs-residuals against estimated distribution Function values. Their distribution should then follow a standard exponential distribution if the model is fit correctly.
2. Plot against estimted cumulative hazard function. This should result in a straight line if the model fits the data. 

# Semi-parametric additive Cox model

# Time discrete Survival models

# Piecewise exponential models (PEM)

### Model equation:

$$
\lambda_i(t|x_i) = \lambda_jexp(x^T\beta), \forall t \in ]a_{j-1}, a_j]
$$

with constant baseline hazards in each of the $J$ intervals.

### Data


```{R, echo = FALSE}
## load Freireich data
data(leuk2, package = "bpcp")
leuk2$treatment <- relevel(leuk2$treatment, ref = "placebo")

## i) Data transformation
## transform the data into piece-wise exponential format
# -> use unique event/censoring times as cut points
event.times <- unique(leuk2$time)

# use split_data function from pammtools package
# devtools::install_github("adibender/pammtools")
leuk.ped <- split_data(Surv(time, status)~., cut=event.times, data=leuk2, id="id")
# dim(leuk.ped)
head(leuk.ped)

```

We fit an intercept-only model for many intervals resulting in many baseline intercepts:

```{R, echo = FALSE}
pem  <- glm(ped_status ~ interval-1, offset=offset, data=leuk.ped, family=poisson(link = log))
summary(pem)

```

* we fit way too many parameters
* intervals which did not face events have super high standard errors and strange coefficients
* -> Two reasons for fitting PAM's with smooth baseline hazards

# Piecewise additive exponential models (PAM)

New compared to PEM: smooth modeling of the piecewise constant baseline hazards e.g. via splines. Cool because:  

* PEM constrained by use of intervals as high $J$ leads to parameter explosion   
* Smoother curves due to penalization of splines on the overlaps of the intervals    
* Problem PEM: no data in interval $]a_{l-1}, a_l]$ -> $\lambda_l = 0$, wiggely hazard rate curves    

### Model equation:

$$
\lambda_i(t|x_i) = exp(f_0(t_j) + x^T\beta) \\
$$
$$
\text{with spline for time dependent baseline hazard: }\\
$$
$$
f_0(t_j) = log(\lambda_0(t_j)) = \sum_{k = 1}^K \gamma_k B_k(t_j) \\
$$
$$
\text{and for time varying covariates: } \\
$$
$$
\lambda_i(t|x_i) = exp(f_0(t_j) + \sum_{j = 1}^pf_k(x_i, k)) 
$$

# Piecewise additive exponential mixed models (PAMM)

### Model equation: 
$$
\lambda_i(t|x_i) = exp(f_0(t_j) + x^T\beta) \\
$$
$$
\text{with spline for time dependent baseline hazard: }\\
$$
$$
f_0(t_j) = log(\lambda_0(t_j)) = \sum_{k = 1}^K \gamma_k B_k(t_j) \\
$$
$$
\text{and for time varying covariates: } \\
$$
$$
\lambda_i(t|x_i) = exp(f_0(t_j) + \sum_{j = 1}^pf_k(x_i, k)) 
$$

### Data

looks like that:

```{r, echo=FALSE, message = FALSE, cache=TRUE}

patients <- readRDS("data/icupatients.Rds")

#### transform data

## ApacheII Score:
# good: 0, bad = 100
# constant, measured only at start of hospital stay
# Effects of such one-time measured measurements often fade out over time 
# because they are changing over time but this change is not accounted for

## i) Transform data
ped <- split_data(Surv(Survdays, PatientDied)~., data=patients, cut=1:30, 
  id="CombinedID")
# filter(patients, PatientDied == 1) %>% slice(2)
# filter(ped, CombinedID == 1113)

ped = ped %>% group_by(interval) %>%
  summarize(n.event = sum(ped_status))
# -> no events prior to interval (4,5] due to exclusion criteria. Start evaluation
# at interval (4,5]

# use zero argument to survival::survSplit
ped <- split_data(Surv(Survdays, PatientDied)~., data=patients, cut=1:30,
  id="CombinedID", zero=4)
# filter(ped, CombinedID == 1113)

# include CombinedicuID as random effect
# 400 hospitals -> would be 400 coeffients as control variable
# frailty model / random effects: only estimate gaussian distribution over
# the hospitals -> only estimate scale parameter / variance
# CHECK  s(CombinedicuID, bs="re"), where "re" means random effects

print(head(ped))

```

Fit a PAMM with a smooth spline term for time (tend) and the other continous variables using this formula:

```{r, echo = TRUE, cache = TRUE}
pamm_icu <- bam(ped_status ~ s(tend) + Year + AdmCatID + DiagID2 + s(Age) + s(BMI) +
      s(ApacheIIScore) + s(CombinedicuID, bs="re"), offset=offset, data = ped,
    family=poisson(), discrete = TRUE)
```

We include the variable CombinedicuID as a random effect aka as a __frailty term__. Therefore wie use ```bs = "re"```. We control for the random effects of the ICU units without having to model a dummy for each of the ICU's. The frailty model just estimates a Gaussian over the different ICU's for which we only have to estimate the variance: 1 parameter vs. 400.

We model the PAM as a Poisson model with log link on the death-indicator ```ped_status```

This is the model summary:
```{r, echo = FALSE}
summary(pamm_icu)
```

### What can we say?

* smooth terms for continuos variables:  
  * if the edf (estimated degress of freedom) = 1, our spline smoother estimated the variable as a linear effect on the hazard rate. This is the case for Age and time  
  * BMI, ApacheIIScore and CombinedicuID (only frailty effect) seem to have a non-linear effect on the hazard rate  
  * \textcolor{red}{HOW TO INTERPRET SPLINE FOR COMBINEDICUID FRAILTY @ ANDREAS}
  * those effects can also be seen graphically which shows the effect of the variable's values on the   \textcolor{red}{linear predictor aka the log(hazard-rate)}. This is the exact value that enters our linear predictor, e.g. 75 year old person enters 0.3
  * time (tend) has a falling slope aka a decreasing effect on the log(hazard) -> has hazard decreases also   
  * ApacheIIScore has almost linear effect: (log-) hazard increases with increasing Apache Scores though this increase is getting lower with higher values of the score  
  * increasing linear age effect, the older, the higher the (log-)hazard  
  * typical shape of the BMI effect, very low BMIs have increased hazard, that decreases toward "normal" BMIs, high uncertainty with respect to effect of very high BMIs as number of patients with respective BMIs decreases (few persons with very high obesity)  
```{r, echo = FALSE}
gg_smooth(x = ped, fit = pamm_icu, terms = c("tend", "Age", "BMI", "ApacheIIScore"))
```
* non-smooth terms for categorical variables:
  * exponentiate the coefficients ```exp(beta)``` and interpret their __mulitplicative__ effect on the hazard rate w.r.t the reference category  
  * example 1: hazard rate for a person treated in 2009 is exp(-0.08622441) = 0.9173883 times as high as the hazard rate for similar person treated in 2007 (reference category)  
  * example 2: hazard rate for a person with Metabolic cancer is exp(-0.92767602) = 0.3954717 times as high as the hazard rate for similar person with Gastrointestinal cancer (reference category)  
  * For more, interpret this table:  
  
```{r, echo = FALSE}
param.beta <- coef(pamm_icu)[2:14]
cbind(beta=param.beta, HR=exp(param.beta))
```



# Frailty models 

# Aalen model

### model equation

$$
\lambda(t) = \lambda_0(t) + x'(t)\beta(t) = \sum_{k = 1}^px_k(t)\beta_k(t)
$$
with additive effects of time-varying covariates on baseline hazard rate

### Data

### Data 

looks like that

```{r, echo = FALSE}
load("data/liver81.RData")

a = c("major_complications", "age", "charlson_score", "sex", "transfusion", 
  "metastasesYN", "major_resection", "days", "status", "id", "metastases")

colnames(liver) = a
head(liver)
```

### Simple additive aalen model form lecture

````{R, echo = FALSE, cache = TRUE}
mod_aa <- aalen(
  formula   = Surv(days, status) ~ age +  charlson_score + major_complications + metastases,
  data      = liver, residuals = 1
)
summary(mod_aa)
```

* \textcolor{red}{DISCUSS interpretation for tests (supremum, Kolmogorov Smirnoff)}
* huhuh


# Cox-Aalen model

### model equation

$$
\lambda(t) = \lambda_0(t) + X(t)\beta(t) \cdot exp(Z(t)'\gamma)
$$
with additive effects of time-varying covariates on baseline hazard rate which are also multiplicatively affected via Cox part of the model. $\gamma$ are time-constant coefficients, PH-assumption, and $\beta$ are time varying additive coefficients by the Aalen-part. 

### Data 

looks like that

```{r, echo = FALSE}
load("data/liver81.RData")

a = c("major_complications", "age", "charlson_score", "sex", "transfusion", 
  "metastasesYN", "major_resection", "days", "status", "id", "metastases")

colnames(liver) = a
head(liver)
```



### What can we say from the graphic?

* Age:
  * the cumulative Hazard of a person aged A+1 at time point t = 1500 is 0.01 higher than that of a person aged A
  * the effect of metastases on the cumulative hazard rate starts to increase t = 1000 after the surgery and is approx. constant before
* Complications:
  * the cumulative Hazard of a person with major complications at time point t = 1500 is 0.2 higher than that of a person without complications
  * the effect of complications on the cumulative hazard rate decreases over time
* Metastases:
  * the cumulative Hazard of a person with metastases at time point t = 2500 is 0.4 higher than that of a person without metastases
  * the effect of metastases on the cumulative hazard rate starts to matter only after t = 1500 and then increases more or less linearly
  * before t = 1500 the effect is non siginificant as the 0 is part of the confidence intervals

Effects for the continous variables estimated as additive via the Aalen-part of the model using the formula ```Surv(days, status) ~ age +  charlson_score + major_complications + metastases + prop(sex) + prop(transfusion) + prop(major_resection), data = liver, residuals = 1, basesim   = 1)```

```{r, echo =FALSE, cache = TRUE}
## Cox-Aalen Model
mod_ca <- cox.aalen(
  formula   = Surv(days, status) ~ age +  charlson_score + major_complications + metastases +
    prop(sex) + prop(transfusion) + prop(major_resection),
  data      = liver, residuals = 1, basesim   = 1)
#layout(matrix(1:4, nrow=2))
plot(mod_ca, specific.comps = c(2:5), las=2)
```

### What can we say from the model summary?

```{r, echo = FALSE}
summary(mod_ca)
```

* Aalen part:
  * Supremum-test: for all 4 variables the H0: no effect can be rejected
  * Kolmogorov Smirnov for time variant effects: H0: constant effect can only clearly be rejected for metastases 
\textcolor{red}{DISCUSS THIS}
* Cox part:
  * sexf: the additive, time-varying effects $\beta(t) = (\beta_{age}(t), \beta_{charlson}(t), \beta_{complications}(t), \beta_{metastases}(t))^T$ from the Aalen model is getting multiplied by factor $exp(0.224) = 1.251071$ for a female compared with a similar man
  * same for transfusion (exp(0.233) = 1.262381) and major_resection (exp(0.254) = 1.289172)
  * \textcolor{red}{DISCUSS}
  
### Cox-Aalen vs. PAM  
  
Compare this with the PAM fitted on the data using the below formula. We explicitly model time varying effects of the 4 variables (metastases, marjo_complications, age, charlson) as in the Aalen model via ti(). 

```
bam(
  formula = ped_status ~ ti(tend,k=10) +
    # use ti() for non-identifiability issue
    metastases + ti(tend, by = as.ordered(metastases),k=10, mc = c(1,0)) +
    major_complications + ti(tend,by = as.ordered(major_complications),k=10, mc = c(1,0)) +
    age + ti(tend, by = age,k=10, mc = c(1,0)) +
    charlson_score + ti(tend, by = charlson_score,k=10, mc = c(1,0)) +
    sex + transfusion + major_resection,
  data   = ped_liver,
  offset = offset,
  family = poisson())
```

The figure below shows the effect of the __time constant variables__ which allow some interpretation:

* NOTE: Constant contributions to time-varying can be interpreted as effects at t=0. Check the model equation and \textcolor{red}{DISCUSS}
* sex: Compared to males, females have a 1.22 times increased risk of experiencing an event (c.p.)
* transfusion: Compared to patients without transfusion, patients with transfustion have a 1.27 times increased risk of experiencing an event (c.p.)
* major resection: A major resection increases the risk of event by a factor of 1.28, compared to patients without a major resection
* \textcolor{red}{DISCUSS} If above interpretation holds, this would fit nicely the effect of the time-constant factors in the Cox-part of above Cox-Aalen model


```{r, echo =FALSE, cache=TRUE}

## PAM
# linear effect of age that varies non-linearly over time
ped_liver <- split_data(Surv(days, status)~., data=liver, id="id")

mod_pam <- bam(
  formula = ped_status ~ ti(tend,k=10) +
    # use ti() for non-identifiability issue
    metastases + ti(tend, by = as.ordered(metastases),k=10, mc = c(1,0)) +
    major_complications + ti(tend,by = as.ordered(major_complications),k=10, mc = c(1,0)) +
    age + ti(tend, by = age,k=10, mc = c(1,0)) +
    charlson_score + ti(tend, by = charlson_score,k=10, mc = c(1,0)) +
    sex + transfusion + major_resection,
  data   = ped_liver,
  offset = offset,
  family = poisson())

## Fixed effects
#tidy_fixed(mod_pam) %>% mutate("exp(coef)" = exp(coef))
# interpret non-time varying effects of 
gg_fixed(mod_pam)

```

Model summary:

```{r, echo = FALSE}
summary(mod_pam)
```

This is the effect estimated for the smooth terms. The total effect of x at time point t is $\beta_x * x + f_x(t)$ where $\beta_x * x$ are the constant effects from the previous graphic and $f_x(t)$ models the effect of the smooth time varying term. Recap the PAM model equation $\lambda_i(t|x_i) = exp(f_0(t_j) + x^T\beta)$ and \textcolor{red}{DISCUSS}. They look like that:

```{r, echo = FALSE, cache = TRUE}
## Smooth effects
pam_smooths <- tidy_smooth(mod_pam)
ggplot(pam_smooths, aes(x=x, y = fit)) +
  geom_stepribbon(aes(ymin = low, ymax = high), alpha = 0.3) +
  geom_step() +
  facet_wrap(~ylab, scale="free_y")
# NOTE: here we visualize the additional time-varying contribution
# The total effect of x, e.g. x = age, will be \beta_x * x + f_x(t)*x
```

Visual comparison of the time-varying effects from Cox-Aalen model on the cumulated Hazard over time (black) vs. the smooth multiplivative effects of the PAM model (red).

```{r, echo = FALSE, cache=TRUE} 
## Visual comparison cumulative (time-varying) coefficients of Cox-Aalen model
# COMPARE: cox.aalen vs. pam 
# step functions in black: additive effects of aalen part of cox.aalen vs.
# red: smooth multiplicative effects of the pam model

# layout(matrix(1:4, nrow=2, byrow=T))
age46_df <- ped_liver %>% ped_info() %>%
  mutate(age = 46) %>% add_cumu_hazard(mod_pam)
age45_df <- ped_liver %>% ped_info() %>%
  mutate(age = 45) %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(2), las = 2)
lines(age46_df$tend, age46_df$cumu_hazard - age45_df$cumu_hazard, col = 2, lwd = 2)

charlson3_df <- ped_liver %>% ped_info() %>%
  mutate(charlson_score = 3) %>% add_cumu_hazard(mod_pam)
charlson2_df <- ped_liver %>% ped_info() %>%
  mutate(charlson_score = 2) %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(3), las = 2)
lines(charlson3_df$tend, charlson3_df$cumu_hazard - charlson2_df$cumu_hazard,
  col = 2, lwd = 2)

metastasesy_df <- ped_liver %>% ped_info() %>%
  mutate(metastases = 'yes') %>% add_cumu_hazard(mod_pam)
metastasesn_df <- ped_liver %>% ped_info() %>%
  mutate(metastases = 'no') %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(5), las = 2)
lines(metastasesy_df$tend, metastasesy_df$cumu_hazard - metastasesn_df$cumu_hazard,
  col = 2, lwd = 2)

mcy_df <- ped_liver %>% ped_info() %>%
  mutate(major_complications = 'yes') %>% add_cumu_hazard(mod_pam)
mcn_df <- ped_liver %>% ped_info() %>%
  mutate(major_complications = 'no') %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(4), las = 2)
lines(mcy_df$tend, mcy_df$cumu_hazard - mcn_df$cumu_hazard, col = 2, lwd = 2)
```


# Competing Risk models

# Random Stuff

### Attest significance only based on $\beta$ and $se(\beta)$

1. compute z-score: $z = \beta / se(\beta)$
2. thresholds for $alpha = 0.05$:
  1. one sided: $z_{thresh} = 1.64$
  1. two sided: $z_{thresh} = 1.96$
3. Reject H0 (coefficient is not significant) if $z > z_{thresh}$

[check their explanation](http://jukebox.esc13.net/untdeveloper/RM/Stats_Module_4/Stats_Module_48.html)


