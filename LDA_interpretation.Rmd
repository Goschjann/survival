---
title: "Survival Data Analysis"
author: "Jann Goschenhofer"
date: "January 2018"
output:
  pdf_document:
    toc: yes
    latex_engine: xelatex
    number_sections: true
  html_document:
    toc: yes
    toc-depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

Summary of models and especially their interpretation (graphically as well as content based) used in Survival Analysis. This document emerged throughout the exam preparation for a lecture on Survival Data Analysis at LMU in winter 2018. Most examples are based on that lecture taught by Prof. Kuechenhoff and Andreas Bender. 



```{r, echo=FALSE, warnings = FALSE, message = FALSE}
# setwd("C:/Users/Martin/Documents/survival-master")

library(ggplot2)
theme_set(theme_bw())
library(data.table)
library(magrittr)
library(pec)
library(broom)
library(tidyr)
library(dplyr)
library(mgcv)
library(purrr)
library(survival)
library(timereg)
# devtools::install_github("adibender/ldatools")
library(ldatools)
# devtools::install_github("adibender/pammtools")
library(pammtools)
library("mgcv")
```

# Big Picture

## Estimation of $S(t)$ and $\lambda(t)$ __without__ Covariate Effects

### Non Parametric

* Kaplan-Meier for $S(t)$  
* Nelson-Aalen for $\Lambda(t)$  
* Breslow for $S(t)$  
* Life-table for $\lambda(t)$  
* Ramlau-Hansen for $\lambda(t)$  

### Parametric

* Assume $T_1, ..., T_n \sim \text{Distribution}(\theta)$ and estimate $\hat \theta = argmin_{\theta}l(\theta)$
* __BUT__ Censoring
* Random Censoring: $L(\theta) = \prod_{i=1}^n \lambda(t_i)^{\delta_i}S(t_i)$

## Regression models __with__ Covariate Effects

### Transformation Models

* model $S(t)$ directly 
* $log(T) = Y = X^T\beta + \sigma \epsilon$
* Assume $\epsilon \sim \text{Distribution}(\theta)$
* Density Transformation: get $F_T(T), f_T(T)$

### Semi-Parametric Cox Model

* PH-Assumption: $\frac{\lambda(t|X_1)}{\lambda(t|X_1)}t$
* model $\lambda(t)$ directly
* $\lambda(t|X) = \lambda_0(t)exp(X^T\beta)$
* __parametric__ Partial Likelihood Estimation: $PL(\beta) = \prod_{i = 1}^m \frac{exp(x_i^T\beta)}{\sum_{j \in R(t_i)}exp(x_i^T\beta)}$
* __non-parametric__ $\lambda_0(t)$ via Breslow, Profile Likelihood for _nuisance parameter_ $\lambda_0(t)$ 
* __BUT1__ effect of $\beta_j$ assumed to be linear, often is not 
* __BUT2__ time varying covariates and effects
* __BUT3__ time constant baseline hazards

#### Semi-parametric Additive Cox Model (__BUT1:__)

* $\lambda_(t|X) = \lambda_0(t)exp(f_1(x_1)\beta_1 + ... + f_k(x_k) \beta_k + x_{k+1}\beta_{k+1} + ... + x_{p}\beta_{p})$
* Estimate $f_j(x_j)$ via splines for smooth nonlinear effects 

#### Time Varying Covariates and Effects (__BUT2:__)

##### Categorical Covariates

Transform short

i       week      arrested     married  emp1  emp2  emp3
------- --------- ------------ -------  ----  ----  ----
1        2        1             0       1     0     NA
2        3        0             1       1     1     1

to long format:

i       week      arrested     married  emp
------- --------- ---------    -------  ----
1       1         0             0       1   
1       2         1             0       0
2       1         0             1       1
2       2         0             1       1
2       3         0             1       1

and fit classic Cox-PH. Equivalent coefficients for both formats __without__ covariates because only events in Partial Likelihood.

##### Continous Variables

* Create artificial time-dependent variable $\tilde x$ and __add__ to classic Cox model  
* e.g.: age and t:age  

##### Effects ?

## Extensions of Cox model

Discretisize time in intervals $[a_o, a_1[, ..., [a_{q-1}, a_q[$


    
### Time Discret Survival Models via GLM's
    
  * Transform data in long format with q time-factors
  * fit GLM (logistic, cloglog, probit) __wihtout intercept__ on event variable as response
  * q coefficients $\beta_{0k}$ for each time interval as some kind of baseline hazard
  * __NICE:__ GLM Toolbox
  * __BUT3:__ No hazard/ Survival interpretation, only Odds etc. 
  
### Piecewise Exponential Models (__BUT3__)
  
* Cox-Model with time varying baseline hazards $\lambda_j$ for $j = 1, ..., q$
* Transform short
  
  
i       $t_i$     $\delta_i$   $x_{i1}$ $x_{i2}$
------- --------- ---------    -------  ----
1       0.25      1             0       3 
2       0.13      0             1       5
  
  
to long formatted __pseudo-data__ :
    
i       $y$       a        $log(\Delta)$ $x_1$  $x_2$
------- --------- ------   -------      ----    ---
1       0         0.1       log(0.1)    0       3    
1       0         0.2       log(0.1)    0       3    
1       1         0.3       log(0.0.5)  0       3    
2       0         0.1       log(0.1)    1       5    
2       0         0.2       log(0.03)   1       5    

* __BUT4:__ exploiding parameters for small intervalls and large $q$
* use __Piecewise Exponential Additive Model__
* __BUT5:__ random effects in the data
* use __Piecewise Exponential Additive Mixed Model__ with Frailty term
* __BUT6:__ only multiplicative effects of the coefficients

## Additive Hazard Regression Models (__BUT6__)

### Aalen Model

* __NICE:__ additive effects
* __NICE:__ new interpretation graphically
* Idea: model effects of covariates on baseline hazard rate $\lambda_0$ __additively__
* Formula: $\lambda(t|X) = \lambda_0(t) + \sum_{k = 1}^p x_k(t)\beta_k(t) = \lambda_0(t) + x^T(t)\beta(t)$


### Cox-Aalen Model

* combine best from both worlds: __additive__ effects on $\lambda_0(t)$ that can be influenced by __multiplicative__ coefficients
* $\lambda(t|X) = \lambda_0(t) + X(t)\beta(t)exp(Z^T(t)\gamma)$
* $\beta(t):$ time varying additive coefficients
* $\gamma:$ time constant multiplicative coefficients. Interpretation: multiplicative effect on hazard if rest kept constant. 
* __BUT7:__ still assumption that $T_i \perp C_i$ 

### Competing Risk Model (But7)

* More than one possible event (e.g.: two types of death) next to censoring of which only one can occur. The events __compete__ with each other as only one of them can occur. 
* Approaches:
  * Seperate "cause-specific" Cox models for each type where the competing events are subsumed in censoring. 
    * Problem 1: assumption, that $T_1 \perp T_2$
    * Problem 2: Kaplan-Meier Curves are biased
  * Cumulative Incidence Curve as solution to problem 2
  * Discretization: Multinomial GLMs






\newpage
# Censoring

1. __Right__:  
    1. Type 1: study ends before event occured. E.g.: fixed time study of 1 year
    2. Type 2: ...
    3. Type 3: person withdraws from study because of other event. E.g.: interest on cancer death, person is getting shot
2. __Left__: we know when event occurs but we do not know when it started. E.g.: person dies at week 4 on cancer but we don't know the time of the disease outbreak. Our observed survival time of 4 weeks is thus equal (best case) or smaller then the observed. 
3. __Left Truncation__: Biased because only people that survived made it to the study. E.g.: deductible in insurances, people with losses < deductibles are not getting observed. 













\newpage
# Kaplan Meier

### Model Equation

Cannot simply $1-F(t)$ due to censoring. KM takes that into account. 

Estimate the __Survival rate__ non-parametrically without any covariables: 

$$
\hat S(t) = \prod_{t_k \leq t}(1-d_k / n_k), \forall t \geq t_1
$$
where $d_k =$ number of events at time point $t_k$ (neither dead nor censored) and $n_k = $ amount of people under risk right before time $t_k$.

Reveals a step function with jumps at each $t_k$ where events took place. 

### Data 

This is some random SOEP data and we estimate Survival functions for both genders: 

```{R, echo = FALSE}

## Read in data
soep <- read.csv("data/soep2000.csv", header = TRUE, sep = ";") # Einlesen Daten
head(soep)

```

### Model

```{R, echo = FALSE}

par(mar=c(5,5,4,4))
## Estimate the survivor function for duration of unemployment separately for
## each gender (Kaplan-Meier)
## Note: Here the event is something positive (end of unemployment),
# thus high survival rates have negative meaning (i.e. staying unemployed longer)
km_sex <- survfit(Surv(dauer, status) ~ female, data = soep)

plot(km_sex, lwd = 2,
  xlab = "Time in months t", las = 1,
  ylab = expression(hat(S)[KM](t)), lty = c(1, 2),
  main = "Duration of unemployment by gender\n (Kaplan-Meier estimator)")

legend("topright", legend = c("male", "female"), lty = c(1, 2))
## Comparison of the curves:
## After how many months are 50% of the persons no longer unemployed?
abline(h = 0.5, lty = 3)
```

This gives incidence for the Proportional Hazards assumption as survival curves are more or less parallel. 


### Test

Plotting estimated confidence intervals __DOES NOT ENABLE__ us to interpret signifiance. KI's can cross, and still there is a significant effect. 

* Only interpret the p-value of the log rank test!
* log rank test resembles the score test in the cox model. 
* ````surfdiff()``` for p = 2 > 1 variables: H0: no differencies across 4 resulting groups. If $p < \alpha:$ reject H0.  

```{R, echo = FALSE}

## Log-rank test for comparing the two groups
# in default mode rho=0 this computes the log-rank test, muchacho
survdiff(Surv(dauer, status) ~ female, data = soep)

```


### By Hand:

1. order events according to time
2. create following table:

$a_{k-1}$   $a_{k}$   $S_k(t)$   $d_k$   $w_k$    $n_k$    $SE(S(t))$    $+/- KI$
--------    -------   --------   ------  -----    -----    ----------    --------
[0,         12[      1          0       1        14       0
[12,        29[       0.917      1       1        12       0.08          [1.06, 0.773]
[29,        31[       0.910      1       0        11       0.087         [1.065, 0.755]


3. where:  
    * $d_k$ = deaths before $k$   
    * $w_k$ = censored before $k$  
    * $n_k = n_{k-1} - d_k - w_k$ people in risk set before $k$  
    * $S(k) = 1 - d_k / n_k$  
 
3. $Var(S(t=k)) = S(t=k)^2 \frac{d_k}{n_k(n_k-d_k)}$
4. $KI = S(k) +/- z_{1-\alpha/2} * \sqrt{Var(S(t=k))} = S(k) +/- 1.96 * \sqrt{Var(S(t=k))}$






# Nelson Aalen









\newpage
# Accelerated Failure Time Transformation models

## General

Assumptions: 

1. covariates have a multiplicative effect on the __Survival time__. E.g.: Survival time for smokers is an accelerated version of the survival time for non-smokers.     
2. the survival time follows an assumed distribution that we get applying a density transformation  

We model the survival time directly with the log-trafo:

$$
log(T) = Y = \beta_0 + X^T\beta + \sigma \epsilon
$$

$$
T = exp(Y) = exp(\beta_0) * exp(X^T\beta) * exp(\sigma \epsilon)
$$
$$
\text{with } \epsilon \sim \text{Distribution e.g.: SEV, Normal, logistic, .....}  
$$
Thus, the effect of the estimated coefficient $\hat\beta_j$ on Survival time T is $exp(\beta_j)$

Steps for the estimation:

1. calculate density for T   
2. classic Maximum Likelihood Estimation  


The exponential and the weibull AFT can be compared with a Cox PH model as they also have proportional (time independent) hazard ratios. This means that the following re-parametrization holds:
$$
\beta_{PH} = - \frac{\beta_{\text{AFT: WB or Exp}}}{\sigma}
$$
where $sigma$ is our scale parameter from the AFT model equation.  __This interpretation goes only from AFT to CoxPH, not in both directions!__. 

Therefore we compare with this baseline Cox model:

```{R, echo = FALSE}
## Cox PH model
coxphFit <- coxph(Surv(futime, fustat) ~ ecog.ps + rx, ovarian)
summary(coxphFit)
```


## Exponential

```{R, echo = FALSE}
## Exponential model
survregExp <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian,
  dist = "exponential")
summary(survregExp)
```

### AFT interpretation:

* Geometric mean of survival time: 1055  
* 1 unit change in ecog.ps shortens survival time by exp(-0.433) = 0.65  
* 1 unit change in rx increases survival time by exp(0.582) = 1.79  
* though, both effects are non significant  

```{R, echo = FALSE}
exp(coef(survregExp))
```

### PH interpretation (invert the coefficients)
* 1 unit change in ecog.ps increases the hazard h(t) by 1/exp(0.433) = 1.54  
* 1 unit change in rx decreases h(t) by 0.56  

```{R, echo = FALSE}
exp(-1 * coef(survregExp))
```


## Weibull

$$
T = exp(Y) = exp(X^T\beta)exp(\sigma \epsilon) \text{, with } \epsilon \sim SEV
$$
using the __density transformation rule__

$$
f_T(T) = f_\epsilon(g^{-1}(T))det|\frac{\partial g^{-1}(T)}{\partial T}|
$$
we can show that $T \sim Weibull(\alpha, \lambda)$ with $\alpha = \frac{1}{\sigma}$ and $\lambda = exp(-X^T\beta)$. Thus: 

* $\lambda(t|X) = \frac{1}{\sigma}t^{\frac{1}{\sigma}-1}exp(X^T\beta)$  
* $S(t|X) = exp(-exp(-X^T\beta)^\frac{1}{\sigma} t^\frac{1}{\sigma})$  


```{R, echo = FALSE}
## Exponential model
survregWB <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian,
  dist = "weibull")
summary(survregWB)
```

### AFT interpretation:

* Geometric mean of survival time: 988 
* 1 unit change in ecog.ps shortens survival time by exp(-0.385) = 0.68  
* 1 unit change in rx increases survival time by exp(0.529) = 1.70 
* though, both effects are non significant  
* If scale parameter ```{R, echo = FALSE} survregWB$scale ``` was close to 1 we would yield an exponential model. Our shape parameter is 1 / scale
* coefficients: ```{R, echo = FALSE} exp(coef(survregWB)) ```


### PH interpretation (multiply by -1 and the shape parameter before exp())
* 1 unit change in ecog.ps increases the hazard h(t) by 1.55 
* 1 unit change in rx decreases h(t) by 0.55

```{R, echo = FALSE}
shapeParameter <- 1 / survregWB$scale
exp(-1 * shapeParameter * coef(survregWB))
```

## Log Normal

Only AFT Interpretation!

* 1 unit incrase in ecog.ps shortens survival time by exp(-.229) = 0.79
* 1 unit incrase in rx increases survival time by exp(0.813) = 2.25
* Can we interpret the scale parameter? Yes, but how?
* \textcolor{red}{MORE TO ADD! DISCUSS}

```{R, echo = FALSE}
## Log Normal model
survregLogNormal <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist = "lognormal")
summary(survregLogNormal)
```

## Log logistic


```{R, echo = FALSE}
## Log Logistic model
survregLogLogistic <- survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist = "loglogistic")
summary(survregLogLogistic)

```













\newpage

# Cox Regression model

Estimates coefficents $\beta$ that have multiplicative effect on time-dependent hazard $\lambda_0(t)$. The baseline hazard is estimated non-parametrically via Breslow estimate. Thus, we yield step-functions for visualization, estimation, ...

[super sweet R-bloggers post on Cox models](https://www.r-bloggers.com/cox-model-assumptions/)


### Model equation

$$
\lambda_i(t) = \lambda_0(t) exp(x_i'\beta)
$$


To get the estimator for the cumulative Hazard and the Survival rate:  

1. estimate $\beta$s via Cox __parametrically__    
1. estimate non-parametrically baseline hazards $\lambda_0(t)$ e.g. via Breslow __non-parametrically__  
1. calculate for each t $\lambda(t) = \lambda_0(t) exp(x_i'\beta)$    
1. cumulate the $\lambda(t)$ to the cumulative Hazards $\Lambda_{t} = \sum_{i = 1}^t \lambda_i$. ```basehaz()``` plottet $\Lambda_0$  
1. calculate estimated Survival $S(t) = exp(-\Lambda_t)$    
Therfore Cox PH model is termed __semi parametric__.   

### Data

where delta depicts the event indicator (delta = 1: non-censored, delta = 0: censored)

```{r, echo = FALSE}

## Read in data
data("tongue", package = "KMsurv")
tongue <- tongue %>% mutate(type = as.factor(type))
head(tongue)
```


### Model

We are searching for the effect of the binary treatment type. 

* Person with type 2 has a multiplicative factor exp(0.4664) = 1.594245 higher hazard rate than a person with type 1 (ceteris paribus in case of other covariates)  
* this effect is not significant as the H0 can not be rejected at $\alpha = 0.05$, REMIND but this does not imply testing of the PH assumption  
* (log rank-) score test: tests for significant differencies in the survival curves for the two   subpopulations seperated by the __categorical variable__ of interest (here: treatment). This means that the probability of an event occurring at any time point is the same for each subpopulation. H0: they do not differ -> p > 0.05: H0 cannot be rejected -> no significant effect of treatment. If there are more than 1 categorical variablewe have the H0: no effect of no covariate at all. Reject again if $p < \alpha$
* ````surfdiff()``` for p = 2 > 1 variables: H0: no differencies across 4 resulting groups. If $p < \alpha:$ reject H0.  
* Partial likelihood test: for __continuos__ variables!
\textcolor{red}{WHAT HAPPENS WITH MORE COVARIATES? E.G.: one significant, the other not}

Summary of the Cox-PH model:

```{r, echo = FALSE}
## Cox-PH model
cox.tongue <- coxph(Surv(time, delta) ~ type, data = tongue)
summary(cox.tongue)

```

### Test the Cox PH assumption for the covariates

#### Graphically

The scaled Schoenfeld residuals are used for that test and plotted against the time. Do this for each covariate to check the PH assumption for each covariate. If they __randomly and unstructured__ center around zero: PH assumption holds! If not, not. The plot estimates a smooth function of the residuals over time for better visualization. Holds here:


```{r, echo =FALSE}
plot(cox.zph(cox.tongue))
```

#### Test PH

Also based on Schoenfeld residuals, not exam-relevant. If $p >> 0.05$ there is no violation of the PH. 

### Test overall fit

Plot Cox-Snell residuals vs. Cumulated Hazard. If they share the diagnonal, everything is fine and we have a good overall model fit. 

```{r, echo = FALSE}
cox.res.cs <- tongue$delta - residuals(cox.tongue, type = "martingale")

cox.res.surv <- survfit(Surv(cox.res.cs, tongue$delta) ~ 1) %>%
  broom::tidy() %>%
  mutate(
    Lambda = -log(estimate),
    F      = pexp(time)) %>%
  rename(cs.residual = time)

## plot empirical F for censored observations vs. cs residuals (and compare to Exp(1))
# ggplot(cox.res.surv, aes(x=cs.residual, y=1-estimate)) +
#   geom_point() +
#   geom_step() +
#   geom_line(aes(y=F), col=2) +
#   ylim(c(0,1)) + ylab(expression(F(r[i]^{CS}))) + xlab(expression(r[i]^{CS}))

## usual visualization of cs residuals vs. cumulative hazard (for censored sample)
ggplot(cox.res.surv, aes(x=cs.residual, y=Lambda)) +
  geom_point() +
  geom_step() +
  geom_abline(intercept=0, slope=1, col=2) + # add bisector (Winkelhalbierende)
  ylab(expression(hat(Lambda)(r[i]^{CS}))) + xlab(expression(r[i]^CS))

```


















\newpage

# Semi-parametric additive Cox model

* $\lambda_(t|X) = \lambda_0(t)exp(f_1(x_1)\beta_1 + ... + f_k(x_k) \beta_k + x_{k+1}\beta_{k+1} + ... + x_{p}\beta_{p})$
* Estimate $f_j(x_j)$ via splines for smooth nonlinear effects 
* Example: age has non-linear effect, smooth age variable via Splines
















\newpage
# Cox model: time varying covariates

Effect remains constant, but covariate varies over time: $\beta x(t)$. For instance, the effect of being employed or unemployed stays constant over time, but the employment status of the people varies over time. 

We convert 

```{R, echo = FALSE}
prison.short = read.csv(file = "data/prisonshort.csv", header = TRUE)
head(prison.short, 4)
```

to long format

```{R, echo = FALSE}
prison.long = read.csv(file = "data/prisonlong.csv", header = TRUE)
head(prison.long, 4)
```

We yield the same Coefficients for both data sets if we include only the time-constant predictors:

```{R, echo = FALSE}
(m1 <- coxph(Surv(week, arrest) ~ fin + age + mar + prio, 
  method = "efron", data = prison.short))

(m2 <- coxph(Surv(start, stop, arrest) ~ fin + age + mar + prio,
  data = prison.long))

```

Now we include the time-varying employment variable and yield a constant effect coefficient for the time varying variable.
```{R, echo = FALSE}
(m3 <- coxph(Surv(start, stop, arrest) ~ fin + age + prio + mar + employed,
  data = prison.long))

```














\newpage
# Cox model: time varying effects

## Idea

Covariate remains constant but the effect changes over time: $\beta(t)\times x$. The same value of $x$ has a different effect at $t_1$ than an $t_2$. Create a fake-time-varying variable, e.g. $log(t)$ and model the coefficients for the interaction of the real variable with the time-dummy. No clear interpretation as coefficients are bound to time-fake-dummy but plot of the effect over time possible.

## Example

Old model with wrong intake of age variable, detected by the schoenfeld test. We assume age to have a time-varying effect on the hazard.

```{R, echo = FALSE}
prisss = prison.long %>%
  mutate(employed.lag1 = lag(employed, default = 0))


m4 <- coxph(Surv(start, stop, arrest) ~ fin + age + prio + mar + employed.lag1, data = prisss)
summary(m4)
cox.zph(m4)
plot(cox.zph(m4)[2])
```

Model age explicitly via an interaction term with time such that $x_{age2} = x_{age}log(t)$. This yields perfect Schoenfeld-plots and an increase in the global p-values of the zph test:

```{R, echo = FALSE}
m5 = coxph(Surv(start, stop, arrest) ~ fin + age + age:I(log(stop))+ prio + mar + employed.lag1, data = prisss)

summary(m5)
cox.zph(m5, transform = "identity")
plot(cox.zph(m5)[2])
plot(cox.zph(m5)[6])
```

We can also test the __PH Assumption__ for a variable this way: if the time varying effect is not significant, the PH is valid. 





















\newpage 

# Model fit Analysis

## Prediction Error Curves (PEC)

The predicted survival time for each time point is compared with the true survival time within the __Brier Score__. Some magic is added such as _inverse probability of censoring weights (IPCW)_ to account for right censoring. Then scores for each time point are computed using Cross-Validation and the Brier Scores over time are plotted for all desired models. The lower the score, the better. This method is __model agnostic__.  

For Melanoma compare predictive performance of Cox model with only variable ulcer as predictor with the reference Kaplan-Meier estimates and a Cox-PH model that uses year as a linear predictor. We see, that our cox-model outperforms the simple Kaplan-Meier estimator (which does not use any variables) and both outperform the stupid Cox model with time as linear predictor.     

```{R, echo = FALSE, message = FALSE, warnings = FALSE}
library(survival)
library(pec)
data("Melanoma", package="MASS")
cox_ulcer <- coxph(Surv(time, status ==1) ~ ulcer, data = Melanoma)
## Estimate CV PEC for even grid of time points between 0 and 4000
pec_obj <- pec::pec(list(cox_ulcer = cox_ulcer),
  formula = Surv(time, status == 1) ~ year + ulcer, 
  data = Melanoma, exact = FALSE, 
  times = seq(0, 4000, by = 200),
  splitMethod = "cv10", B = 20, 
  ipcw.refit = TRUE, 
  reference = TRUE)

mod_list <- list(cox_ulcer = cox_ulcer,
  cox_year = update(cox_ulcer, .~. -ulcer + year))
pec_obj2 <- pec::pec(mod_list, Surv(time,status==1)~year + ulcer,
  data =  Melanoma, times = seq(0,4000, by=200),
  splitMethod = "cv10", B = 20, ipcw.refit = TRUE, exact = FALSE)
plot(pec_obj2)
```


## Residuals

### Schoenfeld Residuals $s_{i,j}$

Use case: test PH assumption for each covariate

Idea: compute Schoenfeld residuals for Variable $k$ and $m$ observations. Those residuals should be independent of the survival time. This is the test that ```cox.zph()``` performs. 

PH: effects of covariates are proportional and thus, time invariant. Thus, check for timely structure in residuals, if some timely structure _is left in the residuals_, the models assumption failed. 

H0: Corr(Schoenfeld-Residuals, Ranking of event times) = 0. If p < 0.05, reject that H0 and the corresponding feature violates the PH assumption as there is still timely structure left in the residuals. 


#### Test

```{R, echo = FALSE, cache = TRUE}

### DATA 
prison.short = read.csv(file = "data/prisonshort.csv", header = TRUE)

prison.long = read.csv(file = "data/prisonlong.csv", header = TRUE)

prisss = prison.long %>%
  mutate(employed.lag1 = lag(employed, default = 0))

m4 <- coxph(Surv(start, stop, arrest) ~ fin + age + prio + mar + employed.lag1, data = prisss)

zph.prison <- cox.zph(m4, transform="identity")
zph.prison
```

Small p-value for variable age indicates problem with the PH assumption here. High value for employed.lag1 indicates nice fulfillment of PH assumption. 

Can we observe this graphically?

#### Graphically

Plot the Schoenfeld residuals for variable age:

```{R, echo = FALSE}

dfage = as.data.frame(cbind(zph.prison$x, zph.prison$y[,2]))
colnames(dfage) = c("time", "ageresidual")

# smallest p-value for age variable, GLOBAL p = 0.094
schaledsch.m4 <- get_scaledsch(m4, transform="identity")
# check age
ggplot(filter(schaledsch.m4, variable=="age"), aes(x=time, y=residual)) +
  geom_point(alpha=0.7) +
  geom_smooth(method="gam", formula=y~s(x)) +
  geom_hline(yintercept = 0, lty=2, col=2) +
  ggtitle(label = "Scaled Schoenfeld residuals for variable age")

```

PH assumption violated because there is non linear structure in the data. 

What can we do?  

1. Exclude variable   
1. __additionally__ model time varying effect as e.g. $x_{age}\cdot log(1+t)$  
1. non-linearly e.g. using splines  

Check variable ```employed lag1``` that had huge p-value in zph test (good sign for PH):

```{R, echo = FALSE}
# check employed lag
ggplot(filter(schaledsch.m4, variable=="employed.lag1"), aes(x=time, y=residual)) +
  geom_point(alpha=0.7) +
  geom_smooth(method="gam", formula=y~s(x)) +
  geom_hline(yintercept = 0, lty=2, col=2) +
  ggtitle(label = "Scaled Schoenfeld residuals for variable employed.lag1")

```

We see what we expected: there seems to be no PH violation. Sweet!

### Martingale residuals $m_i$

$m_i=\delta_i-r_i$
Use case: determine the functional form to be used for a given covariate.

Measure of the difference between expected and observed events. Takes values between $-\infty$ and +1.
$\sum^n_{i=1}m_i=0$
plot residuals against a single covariate. A smoothed fit is used. If the plot is linear no transformation of the covariate is needed. If there is a threshold, a discretization may be in order.

### Deviance residuals $d_i$

Use case: Check for outliers. Assess the effect of a given individual on the model. 

Idea: One could use martingale residuals , but they are highly skewed. The logarithm inflates values of the martingale residual close to 1 and shrinks large negative values. This leads to a more normally shaped distribution. 

Plot $d_i$ versus the risk scores $x'\beta$
(it's from the book page 381. the slides say something different, but it doesn't make a lot of sense imo...)

### Cox Snell residuals $r_i$

Use case: Check overall goodness of fit

#### Graphically 

H0: Model works - Cox-snell residuals should follow an Exp(1) distribution. If the cox-snell-residuals distribution deviates strongly from the Exp(1), the model does not fit well. 

#### Test

```{R, echo = FALSE}

```



#### Model 

#### Example

Check the overall goodness of fit for a simple cox model: 

```{R, echo  = FALSE, cache = TRUE}
data("tongue", package = "KMsurv")
tongue <- tongue %>% mutate(type = as.factor(type))
# or:
#tongue <- read.table("tongue.dat", header = TRUE)
#head(tongue)
#str(tongue)

## Cox-PH model
cox.tongue <- coxph(Surv(time, delta) ~ type, data = tongue)
summary(cox.tongue)

cox.res.cs <- tongue$delta - residuals(cox.tongue, type = "martingale")


# survfit estimate the 1 - F, where F is the empirical distribution estimate for
# the censored sample of cox snell residuals

cox.res.surv <- survfit(Surv(cox.res.cs, tongue$delta) ~ 1) %>%
  broom::tidy() %>%
  mutate(
    Lambda = -log(estimate),
    F      = pexp(time)) %>%
  rename(cs.residual = time)

## plot empirical F for censored observations vs. cs residuals (and compare to Exp(1))
ggplot(cox.res.surv, aes(x=cs.residual, y=1-estimate)) +
  geom_point() +
  geom_step() +
  geom_line(aes(y=F), col=2) +
  ylim(c(0,1)) + ylab(expression(F(r[i]^{CS}))) + xlab(expression(r[i]^{CS}))

## usual visualization of cs residuals vs. cumulative hazard (for censord sample)
ggplot(cox.res.surv, aes(x=cs.residual, y=Lambda)) +
  geom_point() +
  geom_step() +
  geom_abline(intercept=0, slope=1, col=2) + # add bisector (Winkelhalbierende)
  ylab(expression(hat(Lambda)(r[i]^{CS}))) + xlab(expression(r[i]^CS))
```

Two options:
1. Plot cs-residuals against estimated distribution Function values. Their distribution should then follow a standard exponential distribution if the model is fit correctly.
2. Plot against estimted cumulative hazard function. This should result in a straight line if the model fits the data. 

## (Partial) Log Likelihood Ratio Test

Idea: Test reduced model $\beta_0$ against full model $\beta$ and check, which fits better. 

Formally: $H0: C\beta = d$ and $H1: C\beta \neq d$.

In standard R output: reduced model is model with all $\beta_0^T = 0^T$ and the full model is the fitted model. Formally this means $H0: C\beta = 0$ and $H1: C\beta \neq 0$.

Test statistics:
$$
lq = 2 (logPL(\hat\beta) - logPL_{H0}(\hat\beta)) \sim \chi^2_{df}
$$
H0: all coefficients are insignificant.

* $lq > \chi_{df}^2 (1-\alpha) \rightarrow$ reject H0   
* $p < \alpha \rightarrow$ reject H0 aka __$\hat\beta$ is not insignificant__.  

Example:

```{R, echo = FALSE}
## Cox-PH model
cox.tongue <- coxph(Surv(time, delta) ~ type, data = tongue)
summary(cox.tongue)
```

The p-value of the Likelihood ratio test is $0.102 > \alpha = 0.05$: we cannot reject the H0 that the coefficient vector $\beta$ (here with only one coefficient for ```type2```) is equal to 0. This goes in line with the p-value for this coefficient. We have 1df as there is only one coefficient to be tested. Works the same way with additional coefficients. 

## (Log rank) Score test 

Idea: Tests for significant differencies in the survival curves for the two subpopulations seperated by the __categorical variable__ of interest (above: type2). This means that the probability of an event occurring at any time point is the same for each subpopulation. 

* H0: they do not differ
* p > 0.05: H0 cannot be rejected -> no significant effect of type2. 
* If there are more than 1 categorical variable we yield the H0: no effect of no covariate at all. Reject H0 again in favor of significant effects if $p < \alpha$.
* Reject the H0 if $W_L:=\frac{U_Lk^2}{V_l} \geq \chi^2_{1-\alpha}(1)$




















\newpage 
# Time discrete Survival models

Discretize time in intervals $[a_0, a_1[, ..., [a_{q-1}, a_q[$ and fit classic GLM's __without__ an intercept on the transformed data with the event variable as response. The coefficients of the time variables are used as intercepts.

## Data

We add the time variable ```t``` as a factor to our data frame

```{R, echo = FALSE}
# create factor variable of time, that is needed to fit time-discrete models
prison.long$t <- as.factor(prison.long$stop)
head(prison.long)

```

## Model

### Logit 

$$
P(T = t | T \geq t, x) = \frac{exp(\eta)}{1+exp(\eta)}
$$
$$
P(T > t | T \geq t, x) = 1 - P(T = t | T \geq t, x) =  \frac{1}{1+exp(\eta)}
$$
$$
\frac{P(T = t | T \geq t, x)}{P(T > t | T \geq t, x)} = exp(\eta) = exp(\beta_0 + X'\beta) = exp(\beta_0)exp(X'\beta) \textbf{ Continuation ratio}
$$

e.g. $beta_{female} = 0.5 \rightarrow$ Risk of woman to have event in time $T = t$ instead of later ih $T > t$ is factor $exp(0.5) = 1.68$ higher than for men ($x_{female} = 0$). 

$\eta = X'\beta$ linear predictor. 

#### Example

```{R, echo = FALSE}
# fit time-discrete model with logit link function
# interpretation of coefficients as Odds-Ratios possible
m.logit <- glm(arrest ~ -1 + t + fin + age + mar + prio,
  family = binomial(link = "logit"), data = prison.long)
summary(m.logit)
```

The risk to go back to jail for a married person in $T=t$ instead of $T > t$ is $exp(-0.53) = 0.58$ lower than for a not-married person. We always __interpret the continutation ratio in the logit time discrete model__!

The hazard in the logit model follows:
$$
\lambda (t| X_{it} ) = P( y_{it} = 1 | X_{it})  = \frac{ exp( \beta_{0t} + X_{it}^T \beta)}{1 + exp(\beta_{0t} + X_{it}^T \beta)}
$$
and thus the baseline hazard (all other covariates than the time dummy-variable of interest):

$$
\lambda_0 (t; X_{it} = 0) = P( y_{it} = 1 ; X_{it} = 0) = \frac{ exp(\beta_{0t}) }{1 + exp(\beta_{0t} ) }
$$

### Cloglog

#### Model equation

$$
\
$$

Interpetation with respect to the hazard rate. 


```{R, echo = FALSE}
# fit time-discrete model with logit link function
# interpretation of coefficients as Odds-Ratios possible
m.cloglog <- glm(arrest ~ -1 + t + fin + age + mar + prio,
  family = binomial(link = "cloglog"), data = prison.long)
summary(m.cloglog)
```

The hazard rate for a married person is $exp(-0.53) = 0.59 $ times as high as for a an unmarried person with all other covariates held constant. 



## Smooth time variables

We include the time variable via a smoothing spline and yield a sparser model with more or less the same coefficients for our covariates:

```{R, echo = FALSE}
m.smooth.logit <- gam(arrest ~ s(stop) + fin + age + mar + prio,
  family = binomial(link = "logit"), data = prison.long)
summary(m.smooth.logit)
```

We cannot interpret those baseline hazards in a reasonable manner. 

Graphically, we see that the baseline hazards from the blue, spline curve is much smoother and less "outlier-sensitive" (e.g.: time points without events), than from the simple logit model in red:

```{R, echo = FALSE}

zero.data <- data.frame(stop = 1:52, fin = 0, age = 0, mar = 0, prio = 0)
smooth.basehaz <- predict.gam(object = m.smooth.logit, newdata = zero.data,
  type = "response")

plot(1:52, exp(m.logit$coefficients[1:52])/(1+exp(m.logit$coefficients[1:52])), 
  type = "l", lty = 1, col = "red", lwd = 2, ylab = "baseline coefficients", xlab = "time as factor")
lines(1:52, smooth.basehaz, type = "l", lty = 1, col = "blue", lwd = 4)

```

















\newpage

# Piecewise exponential models (PEM)

### Model equation:

$$
\lambda_i(t|x_i) = \lambda_j exp(x^T\beta), \forall t \in ]a_{j-1}, a_j]
$$

with constant baseline hazards in each of the $J$ intervals.

### Data

```{R, echo = FALSE}
## load Freireich data
data(leuk2, package = "bpcp")
leuk2$treatment <- relevel(leuk2$treatment, ref = "placebo")

## i) Data transformation
## transform the data into piece-wise exponential format
# -> use unique event/censoring times as cut points
event.times <- unique(leuk2$time)

# use split_data function from pammtools package
# devtools::install_github("adibender/pammtools")
leuk.ped <- split_data(Surv(time, status)~., cut=event.times, data=leuk2, id="id")
# dim(leuk.ped)
head(leuk.ped)

```

We fit a model for many intervals and the treatment variable resulting in many baseline intercepts:

```{R, echo = FALSE}
pem  <- glm(ped_status ~ interval - 1 + treatment, offset=offset, data=leuk.ped, family=poisson(link = log))
summary(pem)
```

* we fit way too many parameters
* intervals which did not face events have super high standard errors and strange coefficients
* -> Two reasons for fitting PAM's with smooth baseline hazards

















\newpage
# Piecewise additive exponential models (PAM)

New compared to PEM: smooth modeling of the piecewise constant baseline hazards e.g. via splines. Cool because:  

* PEM constrained by use of intervals as high $J$ leads to parameter explosion   
* Smoother curves due to penalization of splines on the overlaps of the intervals    
* Problem PEM: no data in interval $]a_{l-1}, a_l]$ -> $\lambda_l = 0$, wiggely hazard rate curves    

### Model equation:

$$
\lambda_i(t|x_i) = exp(f_0(t_j) + x^T\beta) \\
$$
$$
\text{with spline for time dependent baseline hazard: }\\
$$
$$
f_0(t_j) = log(\lambda_0(t_j)) = \sum_{k = 1}^K \gamma_k B_k(t_j)
$$
$$
\text{and for time varying covariates: } \\
$$
$$
\lambda_i(t|x_i) = exp(f_0(t_j) + \sum_{j = 1}^pf_k(x_i, k)) 
$$

## Model

```{R, echo = FALSE}
pam = gam(ped_status ~ s(tend) + treatment, offset = offset, data = leuk.ped, family = poisson(link = log))
summary(pam)
```

we can also plot the splined piecewise smoothed hazards:

```{R, echo = FALSE}
gg_smooth(x = leuk.ped, fit = pam, terms = c("tend"))
```



















\newpage
# Piecewise additive exponential mixed models (PAMM)

Extension of the PAM with __Frailty terms__. Here: 400 different ICU's and we do not want to control for each of it. Therefore, we fit a smooth, splined version of the frailty. Nice: need only to fit a distribution with shape parameter

### Data

looks like that:

```{r, echo=FALSE, message = FALSE, cache=TRUE}

patients <- readRDS("data/icupatients.Rds")

#### transform data

## ApacheII Score:
# good: 0, bad = 100
# constant, measured only at start of hospital stay
# Effects of such one-time measured measurements often fade out over time 
# because they are changing over time but this change is not accounted for

## i) Transform data
ped <- split_data(Surv(Survdays, PatientDied)~., data=patients, cut=1:30, 
  id="CombinedID")
# filter(patients, PatientDied == 1) %>% slice(2)
# filter(ped, CombinedID == 1113)

ped = ped %>% group_by(interval) %>%
  summarize(n.event = sum(ped_status))
# -> no events prior to interval (4,5] due to exclusion criteria. Start evaluation
# at interval (4,5]

# use zero argument to survival::survSplit
ped <- split_data(Surv(Survdays, PatientDied)~., data=patients, cut=1:30,
  id="CombinedID", zero=4)
# filter(ped, CombinedID == 1113)

# include CombinedicuID as random effect
# 400 hospitals -> would be 400 coeffients as control variable
# frailty model / random effects: only estimate gaussian distribution over
# the hospitals -> only estimate scale parameter / variance
# CHECK  s(CombinedicuID, bs="re"), where "re" means random effects

print(head(ped))

```

Fit a PAMM with a smooth spline term for time (tend) and the other continous variables using this formula:

```{r, echo = TRUE, cache = TRUE}
pamm_icu <- bam(ped_status ~ s(tend) + Year + AdmCatID + DiagID2 + s(Age) + s(BMI) +
      s(ApacheIIScore) + s(CombinedicuID, bs="re"), offset=offset, data = ped,
    family=poisson(), discrete = TRUE)
```

We include the variable CombinedicuID as a random effect aka as a __frailty term__. Therefore wie use ```bs = "re"```. We control for the random effects of the ICU units without having to model a dummy for each of the ICU's. The frailty model just estimates a Gaussian over the different ICU's for which we only have to estimate the variance: 1 parameter instead of 400.

We model the PAM as a Poisson model with log link on the death-indicator ```ped_status```

This is the model summary:
```{r, echo = FALSE}
summary(pamm_icu)
```

### What can we say?

* __smooth terms for continous variables:__  
  * if the edf (estimated degress of freedom) = 1, our spline smoother estimated the variable as a linear effect on the hazard rate. This is the case for Age and time  
  * BMI, ApacheIIScore and CombinedicuID (only frailty effect) seem to have a non-linear effect on the hazard rate  
  * We can and will not interpret the frailty term in detail
  * those effects can also be seen graphically which shows the effect of the variable's values on the \textcolor{red}{linear predictor aka the log(hazard-rate)}. This is the exact value that enters our linear predictor, e.g. 75 year old person enters 0.3
  * time (tend) has a falling slope aka a decreasing effect on the log(hazard) -> hazard decreases also   
  * ApacheIIScore has almost linear effect: (log-) hazard increases with increasing Apache Scores though this increase is getting lower with higher values of the score  
  * increasing linear age effect, the older, the higher the (log-)hazard  
  * typical shape of the BMI effect, very low BMIs have increased hazard, that decreases toward "normal" BMIs, high uncertainty with respect to effect of very high BMIs as number of patients with respective BMIs decreases (few persons with very high obesity)  
  
```{r, echo = FALSE}
gg_smooth(x = ped, fit = pamm_icu, terms = c("tend", "Age", "BMI", "ApacheIIScore"))
```

* __non-smooth terms for categorical variables:__
  * exponentiate the coefficients ```exp(beta)``` and interpret their __mulitplicative__ effect on the hazard rate w.r.t the reference category  
  * example 1: hazard rate for a person treated in 2009 is exp(-0.08622441) = 0.9173883 times as high as the hazard rate for similar person treated in 2007 (reference category)  
  * example 2: hazard rate for a person with Metabolic cancer is exp(-0.92767602) = 0.3954717 times as high as the hazard rate for similar person with Gastrointestinal cancer (reference category)  
  * For more, interpret this table:  
  
```{r, echo = FALSE}
param.beta <- coef(pamm_icu)[2:14]
cbind(beta=param.beta, HR=exp(param.beta))
```



# Frailty models 















\newpage
# Aalen model

Super flexible with __additive__ and not multiplicative (Cox) effects on the hazard. Why flexibel? Because all covariates and coefficients can be time-varying!

### model equation

$$
\lambda(t) = \lambda_0(t) + x'(t)\beta(t) = \lambda_0(t) + \sum_{k = 1}^px_k(t)\beta_k(t)
$$

with additive effects of time-varying covariates on baseline hazard rate

### Data 

```{r, echo = FALSE}
load("data/liver81.RData")

a = c("major_comp", "age", "charlson_score", "sex", "transfusion", 
  "metastasesYN", "major_resection", "days", "status", "id", "metastases")

colnames(liver) = a
head(liver)
```

### Model

````{R, echo = FALSE, cache = TRUE}
mod_aa <- aalen(
  formula   = Surv(days, status) ~ age +  charlson_score + major_comp + metastases,
  data      = liver, residuals = 1
)
summary(mod_aa)
```

* __Supremum-Test__ 
    * if p < 0.05, H0: additive coefficient = 0 can be rejected (all 4 variables __do have significant effect different from 0__, though metastases is not as super significant as the others, check Graphics)
    * if p > 0.05: H0: additive coefficient = 0 __can not__ be rejected
* __Kolmogorov Smirnoff Test__
    * if p < 0.05, H0: constant effect can be rejected (charlson score, mestastasesyes have __time varying, non-constant effect__)
    * if p > 0.05, H0: constant effect can not be rejected (age, major\_complicationyes have __time constant effect_)
* __Graphically:__
    * linear trend over time: constant effect (age, complications)
    * non-linear, disrupted trend over time: time varying effect (metastases, charlson)
    * if 0 in Confidence Interval: insiginificant effect (explains, why $p_{metastases} > p_{others}$)


```{R, echo = FALSE}
layout(matrix(1:2 ,ncol = 2))
plot(mod_aa, specific = c(2:5))
```

### Semi parametric Aalen:

As we now know, that age and major\_comp have constant effects on the hazard, we can model them explicitly like that and yield fixed coefficients:

```{R, echo = FALSE}
mod_aa2 <- aalen(
  formula   = Surv(days, status) ~ const(age) +  charlson_score + const(major_comp) + metastases,
  data      = liver, residuals = 1
)
summary(mod_aa2)
```













\newpage
# Cox-Aalen model

### model equation

$$
\lambda(t) = X(t)\beta(t) \cdot exp(Z(t)'\gamma)
$$
with additive effects of time-varying covariates on baseline hazard rate which are also multiplicatively affected via Cox part of the model. $\gamma$ are time-constant coefficients, PH-assumption, and $\beta$ are time varying additive coefficients by the Aalen-part. 

### Data 

looks like that

```{r, echo = FALSE}
load("data/liver81.RData")

a = c("major_complications", "age", "charlson_score", "sex", "transfusion", 
  "metastasesYN", "major_resection", "days", "status", "id", "metastases")

colnames(liver) = a
head(liver)
```



### What can we say from the graphic?

* Age:
  * the cumulative Hazard of a person aged A+1 at time point t = 1500 is 0.01 higher than that of a person aged A
  * the effect of metastases on the cumulative hazard rate starts to increase t = 1000 after the surgery and is approx. constant before
* Complications:
  * the cumulative Hazard of a person with major complications at time point t = 1500 is 0.2 higher than that of a person without complications
  * the effect of complications on the cumulative hazard rate decreases over time
* Metastases:
  * the cumulative Hazard of a person with metastases at time point t = 2500 is 0.4 higher than that of a person without metastases
  * the effect of metastases on the cumulative hazard rate starts to matter only after t = 1500 and then increases more or less linearly
  * before t = 1500 the effect is non siginificant as the 0 is part of the confidence intervals

Effects for the continous variables estimated as additive via the Aalen-part of the model using the formula ```Surv(days, status) ~ age +  charlson_score + major_complications + metastases + prop(sex) + prop(transfusion) + prop(major_resection), data = liver, residuals = 1, basesim   = 1)```

```{r, echo =FALSE, cache = TRUE}
## Cox-Aalen Model
mod_ca <- cox.aalen(
  formula   = Surv(days, status) ~ age +  charlson_score + major_complications + metastases +
    prop(sex) + prop(transfusion) + prop(major_resection),
  data      = liver, residuals = 1, basesim   = 1)
#layout(matrix(1:4, nrow=2))
plot(mod_ca, specific.comps = c(2:5), las=2)
```

### What can we say from the model summary?

```{r, echo = FALSE}
summary(mod_ca)
```

* Aalen part:
  * Supremum-test: for all 4 variables the _H0: no effect_ can be rejected, all covariables have significant influence
  * Kolmogorov Smirnov for time variant effects: H0: constant effect can only clearly be rejected for metastases, others have constant effects
* Cox part:
  * sexf: the additive, time-varying effects $\beta(t) = (\beta_{age}(t), \beta_{charlson}(t), \beta_{complications}(t), \beta_{metastases}(t))^T$ from the Aalen model is getting multiplied by factor $exp(0.224) = 1.251071$ for a female compared with a similar man __for fixed time t__.
  * same for transfusion (exp(0.233) = 1.262381) and major_resection (exp(0.254) = 1.289172)

### Cox-Aalen vs. PAM  
  
Compare this with the PAM fitted on the data using the below formula. We explicitly model time varying effects of the 4 variables (metastases, marjo_complications, age, charlson) as in the Aalen model via ti(). 

```
bam(
  formula = ped_status ~ ti(tend,k=10) +
    # use ti() for non-identifiability issue
    metastases + ti(tend, by = as.ordered(metastases),k=10, mc = c(1,0)) +
    major_complications + ti(tend,by = as.ordered(major_complications),k=10, mc = c(1,0)) +
    age + ti(tend, by = age,k=10, mc = c(1,0)) +
    charlson_score + ti(tend, by = charlson_score,k=10, mc = c(1,0)) +
    sex + transfusion + major_resection,
  data   = ped_liver,
  offset = offset,
  family = poisson())
```

The figure below shows the effect of the __time constant variables__ which allow some interpretation:

* NOTE: Constant contributions to time-varying can be interpreted as effects at t=0. Check the model equation and \textcolor{red}{DISCUSS}
* sex: Compared to males, females have a 1.22 times increased risk of experiencing an event (c.p.)
* transfusion: Compared to patients without transfusion, patients with transfustion have a 1.27 times increased risk of experiencing an event (c.p.)
* major resection: A major resection increases the risk of event by a factor of 1.28, compared to patients without a major resection
* \textcolor{red}{DISCUSS} If above interpretation holds, this would fit nicely the effect of the time-constant factors in the Cox-part of above Cox-Aalen model


```{r, echo =FALSE, cache=TRUE}

## PAM
# linear effect of age that varies non-linearly over time
ped_liver <- split_data(Surv(days, status)~., data=liver, id="id")

mod_pam <- bam(
  formula = ped_status ~ ti(tend,k=10) +
    # use ti() for non-identifiability issue
    metastases + ti(tend, by = as.ordered(metastases),k=10, mc = c(1,0)) +
    major_complications + ti(tend,by = as.ordered(major_complications),k=10, mc = c(1,0)) +
    age + ti(tend, by = age,k=10, mc = c(1,0)) +
    charlson_score + ti(tend, by = charlson_score,k=10, mc = c(1,0)) +
    sex + transfusion + major_resection,
  data   = ped_liver,
  offset = offset,
  family = poisson())

## Fixed effects
#tidy_fixed(mod_pam) %>% mutate("exp(coef)" = exp(coef))
# interpret non-time varying effects of 
gg_fixed(mod_pam)

```

Model summary:

```{r, echo = FALSE}
summary(mod_pam)
```

This is the effect estimated for the smooth terms. The total effect of x at time point t is $\beta_x * x + f_x(t)$ where $\beta_x * x$ are the constant effects from the previous graphic and $f_x(t)$ models the effect of the smooth time varying term. Recap the PAM model equation $\lambda_i(t|x_i) = exp(f_0(t_j) + x^T\beta)$ and \textcolor{red}{DISCUSS}. They look like that:

```{r, echo = FALSE, cache = TRUE}
## Smooth effects
pam_smooths <- tidy_smooth(mod_pam)
ggplot(pam_smooths, aes(x=x, y = fit)) +
  geom_stepribbon(aes(ymin = low, ymax = high), alpha = 0.3) +
  geom_step() +
  facet_wrap(~ylab, scale="free_y")
# NOTE: here we visualize the additional time-varying contribution
# The total effect of x, e.g. x = age, will be \beta_x * x + f_x(t)*x
```

Visual comparison of the time-varying effects from Cox-Aalen model on the cumulated Hazard over time (black) vs. the smooth multiplivative effects of the PAM model (red).

```{r, echo = FALSE, cache=TRUE} 
## Visual comparison cumulative (time-varying) coefficients of Cox-Aalen model
# COMPARE: cox.aalen vs. pam 
# step functions in black: additive effects of aalen part of cox.aalen vs.
# red: smooth multiplicative effects of the pam model

# layout(matrix(1:4, nrow=2, byrow=T))
age46_df <- ped_liver %>% ped_info() %>%
  mutate(age = 46) %>% add_cumu_hazard(mod_pam)
age45_df <- ped_liver %>% ped_info() %>%
  mutate(age = 45) %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(2), las = 2)
lines(age46_df$tend, age46_df$cumu_hazard - age45_df$cumu_hazard, col = 2, lwd = 2)

charlson3_df <- ped_liver %>% ped_info() %>%
  mutate(charlson_score = 3) %>% add_cumu_hazard(mod_pam)
charlson2_df <- ped_liver %>% ped_info() %>%
  mutate(charlson_score = 2) %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(3), las = 2)
lines(charlson3_df$tend, charlson3_df$cumu_hazard - charlson2_df$cumu_hazard,
  col = 2, lwd = 2)

metastasesy_df <- ped_liver %>% ped_info() %>%
  mutate(metastases = 'yes') %>% add_cumu_hazard(mod_pam)
metastasesn_df <- ped_liver %>% ped_info() %>%
  mutate(metastases = 'no') %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(5), las = 2)
lines(metastasesy_df$tend, metastasesy_df$cumu_hazard - metastasesn_df$cumu_hazard,
  col = 2, lwd = 2)

mcy_df <- ped_liver %>% ped_info() %>%
  mutate(major_complications = 'yes') %>% add_cumu_hazard(mod_pam)
mcn_df <- ped_liver %>% ped_info() %>%
  mutate(major_complications = 'no') %>% add_cumu_hazard(mod_pam)
plot(mod_ca, specific.comp = c(4), las = 2)
lines(mcy_df$tend, mcy_df$cumu_hazard - mcn_df$cumu_hazard, col = 2, lwd = 2)
```















\newpage

# Competing Risk models

* More than one possible event (e.g.: two types of death) next to censoring of which only one can occur. The events __compete__ with each other as only one of them can occur. 
* Problem with Survival rate estimates (such as KM):
  * Soldiers can die in combat or by accident
  * All 100 soldiers die in helicopter accident at time t __before__ they could take part in combat
  * Nobody died in combat at t $\rightarrow S_{Combat}(t) = P(T_{Combat} > t) = 1$ though no combat took place
  * for Kaplan Meier: P(T_{Combat} = t) undefined because nobody at risk at time t.
  * $\Rightarrow$ difficult interpretation of Survival Curves in competing risk scenario
* Approaches:
  * Seperate "cause-specific" Cox models for each type where the competing events are subsumed in censoring. 
    * Problem 1: assumption, that $T_1 \perp T_2$
    * Problem 2: Kaplan-Meier Curves are biased
  * Cumulative Incidence Curve as solution to problem 2
  * Discretization: Multinomial GLMs
  
## Cause-specific Cox PH Models

* One Cox model for each cause. 
* Interpretation based on non-occurence of competing events
* Estimate via Partial Likelihood
* Treat competing events $-j$ as being censored which is again the unrealistic independence assumption 

$$
\lambda_j(t) = \lambda_{0j}(t)exp(X^T\beta_j) \text{ with possibly cause-specific coefficients } \beta_{j}  
$$
  
## Cumulative Incidence Curves

### Problem

Study with 100 people over 5 months. Two possible deats: Virus or Cancer. 99 patients die t = 3 on V, 1 dies at t = 5 on C. What is survival rate at t=5 $S(t = 5)$? Depending on the interpretation of V:    

1. they represent the C-subpopulation and would have died on Cancer also: $S(t = 5)$ = (1-1)/1 = 0? Thus $Risk_{C1}(T = 5) = 1$ which is the classic Kaplan-Meier way
2. they would have survived Cancer: $S(t = 5) = 1-0.01 = 0.99$. Thus $Risk_{C2}(T = 5) = 0.01$ also termed  __marginal probability__ as V-patients are understood as Cancer-Survivors

We would like to know, who of the V-deaths would have died on Cancer in case they survived V. Which of both Risks is more informative?

### Howto CIC

1. Estimate hazard at ordered failure times $t_f$ for event-type $j$ of interest:
$$
\hat \lambda(t_f)) = \frac{m_{jf}}{n_f} = \frac{\text{\# events j at $t_f$}}{\text{\# subjects at risk at $st_f$}} 
$$
2. Estimate __overall Survival Probability for all event-types__ $\hat S(t_{f-1})$
3. Compute estimated incidence of failing at time $t_f$ from event type c:
$$
\hat I_{jf} = \hat S(t_{f-1}) \times \hat \lambda_j(t_f) 
$$
4. Cumulate:



$$
\text{CIC}_j(t_f) = P(T \leq f| C = j) = \sum_{l = 1}^f \hat S(t_l) \times \hat \lambda_j(t_l) 
$$

Also termed \textbf{Aalen-Johannsen Estimator of cumulative incidence}. Kaplan Meier would use event dependent $\hat S_c(t_{l-1})$ instead of overall $\hat S(t_{l-1})$



## Multinomial time-discret models

Discretize time in q intervals $[a_0, a_1[, ..., [a_{q-1}, a_q[$


$$
\lambda_j(t|X) = P(T = t, C = j, T \geq t, X) = \frac{exp(\beta_{0tj} + X^T\beta_j)}{1 + \sum_{i = 1}^k exp(\beta_{0ti} + X^T\beta_i)} \text{ with \# different events = k + 1}
$$
$$
\lambda_0(t|X) = P(T > t, C = j, T\geq t, X) = \frac{1}{1 + \sum_{i = 1}^k exp(\beta_{0ti} + X^T\beta_i)}
$$
Interpretation by cause specific log odds w.r.t. reference event type:

$$
log \frac{\lambda_j(t|X)}{\lambda_0(t|X)} = \beta_{0tj} + X^T\beta_{j}
$$
$$
\frac{\lambda_j(t|X)}{\lambda_0(t|X)} = exp(\beta_{0tj})exp(X^T\beta_{j})
$$


and $exp(\beta_lj)$ = the effect of covariate $x_l$ on cause specific hazard w.r.t nothing else happens



















\newpage
# Function calls

#### Cox PH

```coxph(Surv(start, stop, arrest) ~ fin + age + mar + employed.lag1, data = prison.long) ```

$\lambda (t|x) = \lambda_0(t)exp(\beta_{fin}x_{fin} + \beta_{age}x_{age} + \beta_{mar}x_{mar} + ...)$

#### Cox PH with time varying effect

```coxph(Surv(start, stop, arrest) ~ fin + age + mar + age:I(log(t)) + employed.lag1, data = prison.long) ```

$\lambda (t|x) = \lambda_0(t)exp(\beta_{fin}x_{fin} + \beta_{age1}x_{age} + \beta_{age2}(x_{age}log(t)) + \beta_{mar}x_{mar} + ...)$


#### Aalen Model

##### All time varying

```aalen(Surv(days, status) ~ age +  charlson_score + major_complications + metastases, data = liver)```

$$
\lambda(t|X) = \beta_0(t) + \beta_{age}(t)*x_{age}(t) +  \beta_{charlson}(t)*x_{charlson}(t) + \beta_{complications}(t)*x_{complications}(t) + \beta_{metastases}(t)*x_{metastases}(t)
$$
##### Semi parametric

```aalen(Surv(days, status) ~ const(age) +  charlson_score + const(major_complications) + metastases, data = liver)```

$$
\lambda(t|X) = \beta_0(t) + \beta_{age}*x_{age}(t) +  \beta_{charlson}(t)*x_{charlson}(t) + \beta_{complications}*x_{complications}(t) + \beta_{metastases}(t)*x_{metastases}(t)
$$

##### Cox Aalen model

``` cox.aalen(formula   = Surv(days, status) ~ age +  charlson_score + major_complications + metastases + prop(sex) + prop(transfusion) + prop(major_resection), data = liver, residuals = 1, basesim   = 1)```

$$
\lambda(t|X) = [\beta_0(t) + \beta_{age}*x_{age}(t) +  \beta_{charlson}(t)*x_{charlson}(t) + \beta_{complications}*x_{complications}(t) + \beta_{metastases}(t)*x_{metastases}(t)] exp(x_{sex}(t)\beta_{sex} + x_{transfusion}\beta_{transfusion} + x_{major resection}(t)\beta_{major resection})
$$

##### PEM

```glm(ped_status ~ interval -1 + treatment, offset = offset, data = leuk.ped, family = poisson(link = log))```

$$
\lambda_i(t|x_i) = exp(\lambda_{0j} + x_{i, treatment}*\beta_{treatment}) 
$$


##### PAM

```gam(ped_status ~ s(tend, k = 10) + treatment, offset = offset, data = leuk.ped, family = poisson(link = log))```

$$
\lambda_i(t|x_i) = exp(f_0(t_j) + x_{i, treatment}*\beta_{treatment}) \\= exp( \sum_{k = 1}^{K=10} \gamma_k B_k(t_j) + x_{i, treatment}*\beta_{treatment})
$$
##### PAMM

```pamm_icu <- bam(ped_status ~ s(tend) + Year + AdmCatID + DiagID2 + s(Age) + s(BMI) + s(ApacheIIScore) + s(CombinedicuID, bs="re"), offset=offset, data = ped, family=poisson(), discrete = TRUE)```

$$
\lambda_i(t|x_i) = exp(f_0(t_j) + x_{i, year}*\beta_{year}) + x_{i, AdmCatID}*\beta_{AdmCatID}) + \\
+ x_{i, DiagID2}*\beta_{DiagID2}) + f(x_{i, age}, t_j) + f(x_{i, bmi}, t_j) + \\
+ f(x_{i, apachescore}, t_j) + b_{frailty})
$$






\newpage
# Random Stuff

### Attest significance only based on $\beta$ and $se(\beta)$

1. compute z-score: $z = \beta / se(\beta)$
2. thresholds for $alpha = 0.05$:
  1. one sided: $z_{thresh} = 1.64$
  1. two sided: $z_{thresh} = 1.96$
3. Reject H0 (coefficient = 0 aka is not significant) if $z > z_{thresh}$

[check their explanation](http://jukebox.esc13.net/untdeveloper/RM/Stats_Module_4/Stats_Module_48.html)


